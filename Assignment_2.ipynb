{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1443cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39ad91",
   "metadata": {},
   "source": [
    "# Assignment 2: Analyzing large datasets with Spark.\n",
    "\n",
    "\n",
    "For this assignment, you will need to make sure you're running from a PySpark docker environment I introduced in class. You can start the docker pySpark docker environment using the following command:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "Make sure you run the command from the directory containing this jupyter notebook and your data folder.\n",
    "\n",
    "\n",
    "</b>\n",
    "# WARNING: For some reason, ipynbb document didn't always sync properly when I was pushing to github. As such, please push often and make sure your incremental changes appear on GitHub.\n",
    "</b>\n",
    "\n",
    "Name: Rexter Delos Santos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0d3e6",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "The first part will use Spark to analyze the following books, which I have downloaded for you to use from Project Gutenberg. The files are saved to the data folder.\n",
    "\n",
    "| File name | Book Title|\n",
    "|:---------:|:----------|\n",
    "|43.txt | The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson|\n",
    "|84.txt | Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley |\n",
    "|398.txt  | The First Book of Adam and Eve by Rutherford Hayes Platt|\n",
    "|3296.txt | The Confessions of St. Augustine by Bishop of Hippo Saint Augustine|\n",
    "\n",
    "The objective is to explore whether we can detect similarity between books within the same topic using word-based similarity. \n",
    "\n",
    "The task of identifying similar texts in Natural Language Processing is crucial. A naive method for determining whether two documents are similar is to treat them as collections of words (bag of words) and use the number of words they share as a proxy for their similarity. It makes sense that two books with religion as the topic (e.g.  `398.txt` and `3296.txt`) would have more words in common than a book that discusses religion and a book that discusses science fiction (e.g. books `84.txt` and `398.txt`). \n",
    "\n",
    "As mentioned above, we will be using Spark to analyze the data. Although Spark is not needed for such a small example, the platform would be ideal for analyzing very large collections of documents, like those often analyzed by large corporations\n",
    "\n",
    "This part of the assignment will rely exclusively on RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15530",
   "metadata": {},
   "source": [
    "### Q1. \n",
    "Start by importing Spark and making sure your environment is set up properly for the assignment.\n",
    "\n",
    "Import the spark context necessary to load a document as an RDD; ignore any error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80df4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/03 21:05:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d5c9d",
   "metadata": {},
   "source": [
    "### Q2 \n",
    "\n",
    "Read in the file `43.txt` as a spark RDD and save it to a variable called `book_43`\n",
    " * make sure `book_43` is of type MapPartitionsRDD, i.e.,\n",
    "   * str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\" should return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4164f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "book_43 = sc.textFile('data/43.txt')\n",
    "\n",
    "str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c051e",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "How many lines does `book_43` contain?\n",
    "* You can only use operations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c5a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "print(f\"{book_43.count()} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2bc6d",
   "metadata": {},
   "source": [
    "### Q4 \n",
    "\n",
    "Prior to analyzing the words contained in this book, we need to first remove the occurrences of non-alphabetical characters and numbers from the text. You can use the following function, which takes a line as input, removes digits and non-word characters, and splits it into a collection of words. \n",
    "\n",
    "```python\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "```\n",
    "\n",
    "Use the fucntion above on the variable (test_line) to see what it returns.\n",
    "```python\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ce2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631902c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS',\n",
       " 'IS',\n",
       " 'AN',\n",
       " 'EXAMPLE',\n",
       " 'OF',\n",
       " 'THAT',\n",
       " 'CONTAINS',\n",
       " 'AND',\n",
       " 'A',\n",
       " 'DASH',\n",
       " 'CONTAINING',\n",
       " 'NUMBER']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_split_line(test_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e951e5",
   "metadata": {},
   "source": [
    "### Q5\n",
    "\n",
    "How many words does `book_43` contain? To answer this question, you may find it useful to apply the function in a spark-fashion. \n",
    "* You can only use operations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c1b93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29116 words\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "word_list = book_43.flatMap(clean_split_line)\n",
    "print(f\"{word_list.count()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d62952",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "How many of the words in book_43 are unique? Given that words can appear in lower, upper or mixed case (ex. The, THE, the), make sure you convert the words into lower case before counting them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07785c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347 words are unique\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "book_43_counts = word_list.map(lambda x: (x.lower(), 1)).reduceByKey(add)\n",
    "# take only words that occurs once (unique)\n",
    "unique_list = book_43_counts.filter(lambda x: x[1] == 1)\n",
    "\n",
    "print(f\"{unique_list.count()} words are unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b13b79",
   "metadata": {},
   "source": [
    "### Q7\n",
    "\n",
    "* Generate an `RDD` that contains the frequency of each word in `book_43`. Call the variable `book_43_counts`. Each item in the `RDD` should be a tuple with the word as the first element of the tuple and the count as the second item of the tuple. The collection should look like the following:\n",
    "\n",
    "[('project', 88), (\"the\", 1807), ... ]\n",
    "\n",
    "* Such a collection may contain a large number of words and it would be imprudent to transfer all the words onto the same machine to display it. Instead, to explore the content of such a collection, display only the first element in your list. \n",
    "\n",
    "* Given the random nature of this operation, the first element element displayed may be different. The first entry for me was:\n",
    "```\n",
    "[('project', 88)]\n",
    "```\n",
    "\n",
    "* You can only use operations or actions to answer the question. \n",
    "* Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "* Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42729b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('project', 88)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "book_43_counts.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4534c",
   "metadata": {},
   "source": [
    "### Q8\n",
    "\n",
    "Sort `book_43_counts` and print the 20 most frequent words in book_43. \n",
    "  * Hint: function `sortByKey` sorts a collection of tuples on the first element element of the list. You can easily change the order of the items in each element and use `sortByKey` to sort on the second item of each element in `book_43_counts`\n",
    "  * You can only use operations or actions to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51b8ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1807, 'the'),\n",
       " (1068, 'of'),\n",
       " (1043, 'and'),\n",
       " (726, 'to'),\n",
       " (686, 'a'),\n",
       " (646, 'i'),\n",
       " (485, 'in'),\n",
       " (471, 'was'),\n",
       " (392, 'that'),\n",
       " (384, 'he'),\n",
       " (378, 'it'),\n",
       " (312, 'you'),\n",
       " (308, 'my'),\n",
       " (301, 'with'),\n",
       " (285, 'his'),\n",
       " (244, 'had'),\n",
       " (203, 'as'),\n",
       " (202, 'for'),\n",
       " (195, 'this'),\n",
       " (193, 'but')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "# book_43_counts.sortBy(lambda x: x[1], ascending=False).take(20)\n",
    "book_43_counts.map(lambda x: (x[1], x[0])).sortByKey(False).take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420b09",
   "metadata": {},
   "source": [
    "### Q9\n",
    "\n",
    "You must have noted that the most frequent words in `book_43_counts` include stop words such as `of`, `the`, `and`, etc.\n",
    "\n",
    "It would be inefficient to compare documents based on whether or not they contain stop words; those are common to all documents. As such, it's common to remove such stop words. The librarary `sklearn.feature_extraction` provides access to a collection of English stop words, which can be loaded using the following snippet:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stop_words.ENGLISH_STOP_WORDS\n",
    "```\n",
    "\n",
    "* Explore ENGLISH_STOP_WORDS (it's a frozen set data structure, i.e., a set that you cannot modify) by printing any 10 words from it. \n",
    " * Hint: convert the frozen set to something you can subscript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0ad996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['often',\n",
       " 'name',\n",
       " 'he',\n",
       " 'least',\n",
       " 'such',\n",
       " 'serious',\n",
       " 'rather',\n",
       " 'alone',\n",
       " 'something',\n",
       " 'thereby']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "stop_words = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625eb521",
   "metadata": {},
   "source": [
    "### Q10\n",
    "\n",
    "Filter out the words in `book_43_counts` by removing those that appear in the ENGLISH_STOP_WORDS.\n",
    "Save the results to a new variable called `book_43_counts_filtered`\n",
    "  * You can only use operarations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses function such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5602b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 88),\n",
       " ('gutenberg', 98),\n",
       " ('ebook', 13),\n",
       " ('strange', 24),\n",
       " ('mr', 128),\n",
       " ('hyde', 105),\n",
       " ('robert', 3),\n",
       " ('stevenson', 3),\n",
       " ('use', 19),\n",
       " ('united', 16),\n",
       " ('world', 9),\n",
       " ('restrictions', 3),\n",
       " ('whatsoever', 2),\n",
       " ('away', 21),\n",
       " ('online', 4),\n",
       " ('www', 10),\n",
       " ('org', 10),\n",
       " ('check', 4),\n",
       " ('country', 5),\n",
       " ('using', 6),\n",
       " ('title', 1),\n",
       " ('october', 2),\n",
       " ('language', 2),\n",
       " ('set', 34),\n",
       " ('encoding', 1),\n",
       " ('utf', 1),\n",
       " ('produced', 2),\n",
       " ('widger', 1),\n",
       " ('start', 6),\n",
       " ('contents', 11),\n",
       " ('story', 8),\n",
       " ('search', 5),\n",
       " ('quite', 17),\n",
       " ('carew', 5),\n",
       " ('murder', 10),\n",
       " ('letter', 16),\n",
       " ('night', 32),\n",
       " ('s', 156),\n",
       " ('narrative', 5),\n",
       " ('henry', 30),\n",
       " ('statement', 3),\n",
       " ('utterson', 131),\n",
       " ('rugged', 1),\n",
       " ('cold', 7),\n",
       " ('scanty', 1),\n",
       " ('discourse', 1),\n",
       " ('backward', 3),\n",
       " ('long', 30),\n",
       " ('dusty', 2),\n",
       " ('friendly', 2),\n",
       " ('meetings', 1),\n",
       " ('wine', 9),\n",
       " ('human', 6),\n",
       " ('beaconed', 1),\n",
       " ('way', 34),\n",
       " ('spoke', 8),\n",
       " ('symbols', 1),\n",
       " ('loudly', 2),\n",
       " ('acts', 2),\n",
       " ('drank', 6),\n",
       " ('gin', 2),\n",
       " ('crossed', 3),\n",
       " ('years', 11),\n",
       " ('approved', 2),\n",
       " ('tolerance', 1),\n",
       " ('envy', 1),\n",
       " ('high', 10),\n",
       " ('pressure', 2),\n",
       " ('spirits', 6),\n",
       " ('inclined', 3),\n",
       " ('help', 9),\n",
       " ('heresy', 1),\n",
       " ('used', 12),\n",
       " ('say', 25),\n",
       " ('quaintly', 1),\n",
       " ('let', 16),\n",
       " ('brother', 1),\n",
       " ('reputable', 2),\n",
       " ('good', 45),\n",
       " ('influence', 4),\n",
       " ('downgoing', 1),\n",
       " ('came', 35),\n",
       " ('chambers', 3),\n",
       " ('shade', 1),\n",
       " ('change', 15),\n",
       " ('feat', 1),\n",
       " ('undemonstrative', 1),\n",
       " ('best', 6),\n",
       " ('similar', 1),\n",
       " ('catholicity', 1),\n",
       " ('mark', 4),\n",
       " ('modest', 1),\n",
       " ('circle', 2),\n",
       " ('ready', 6),\n",
       " ('opportunity', 2),\n",
       " ('known', 9),\n",
       " ('longest', 1),\n",
       " ('like', 59),\n",
       " ('growth', 2),\n",
       " ('implied', 3),\n",
       " ('aptness', 1),\n",
       " ('distant', 1),\n",
       " ('kinsman', 2),\n",
       " ('nut', 1),\n",
       " ('crack', 1),\n",
       " ('subject', 6),\n",
       " ('common', 7),\n",
       " ('reported', 2),\n",
       " ('walks', 2),\n",
       " ('said', 130),\n",
       " ('looked', 24),\n",
       " ('dull', 1),\n",
       " ('appearance', 7),\n",
       " ('greatest', 1),\n",
       " ('store', 1),\n",
       " ('chief', 3),\n",
       " ('week', 8),\n",
       " ('aside', 3),\n",
       " ('occasions', 1),\n",
       " ('calls', 2),\n",
       " ('business', 16),\n",
       " ('chanced', 4),\n",
       " ('rambles', 1),\n",
       " ('led', 10),\n",
       " ('quarter', 3),\n",
       " ('quiet', 5),\n",
       " ('drove', 5),\n",
       " ('trade', 1),\n",
       " ('weekdays', 1),\n",
       " ('inhabitants', 1),\n",
       " ('emulously', 1),\n",
       " ('grains', 1),\n",
       " ('coquetry', 1),\n",
       " ('shop', 3),\n",
       " ('fronts', 1),\n",
       " ('stood', 14),\n",
       " ('invitation', 2),\n",
       " ('smiling', 3),\n",
       " ('saleswomen', 1),\n",
       " ('veiled', 1),\n",
       " ('charms', 1),\n",
       " ('contrast', 1),\n",
       " ('dingy', 3),\n",
       " ('neighbourhood', 2),\n",
       " ('painted', 1),\n",
       " ('brasses', 1),\n",
       " ('cleanliness', 1),\n",
       " ('caught', 3),\n",
       " ('pleased', 3),\n",
       " ('passenger', 2),\n",
       " ('east', 1),\n",
       " ('line', 1),\n",
       " ('just', 11),\n",
       " ('point', 12),\n",
       " ('certain', 15),\n",
       " ('block', 2),\n",
       " ('gable', 1),\n",
       " ('showed', 4),\n",
       " ('lower', 3),\n",
       " ('storey', 3),\n",
       " ('discoloured', 1),\n",
       " ('feature', 1),\n",
       " ('prolonged', 2),\n",
       " ('negligence', 2),\n",
       " ('equipped', 1),\n",
       " ('blistered', 1),\n",
       " ('tramps', 1),\n",
       " ('slouched', 1),\n",
       " ('panels', 2),\n",
       " ('kept', 8),\n",
       " ('steps', 7),\n",
       " ('schoolboy', 2),\n",
       " ('tried', 2),\n",
       " ('close', 7),\n",
       " ('generation', 1),\n",
       " ('repair', 1),\n",
       " ('abreast', 1),\n",
       " ('lifted', 4),\n",
       " ('pointed', 2),\n",
       " ('companion', 4),\n",
       " ('affirmative', 1),\n",
       " ('mind', 26),\n",
       " ('added', 12),\n",
       " ('home', 15),\n",
       " ('end', 22),\n",
       " ('literally', 1),\n",
       " ('seen', 18),\n",
       " ('folks', 1),\n",
       " ('asleep', 2),\n",
       " ('procession', 2),\n",
       " ('till', 7),\n",
       " ('got', 8),\n",
       " ('state', 9),\n",
       " ('listens', 2),\n",
       " ('begins', 2),\n",
       " ('policeman', 2),\n",
       " ('figures', 1),\n",
       " ('stumping', 1),\n",
       " ('eastward', 1),\n",
       " ('walk', 7),\n",
       " ('maybe', 1),\n",
       " ('able', 3),\n",
       " ('cross', 2),\n",
       " ('ran', 8),\n",
       " ('horrible', 2),\n",
       " ('child', 14),\n",
       " ('ground', 5),\n",
       " ('sounds', 7),\n",
       " ('hear', 10),\n",
       " ('hellish', 2),\n",
       " ('wasn', 1),\n",
       " ('damned', 2),\n",
       " ('gave', 13),\n",
       " ('took', 15),\n",
       " ('collared', 1),\n",
       " ('gentleman', 13),\n",
       " ('group', 2),\n",
       " ('perfectly', 2),\n",
       " ('look', 15),\n",
       " ('turned', 12),\n",
       " ('family', 6),\n",
       " ('pretty', 4),\n",
       " ('doctor', 39),\n",
       " ('sent', 8),\n",
       " ('worse', 8),\n",
       " ('frightened', 3),\n",
       " ('curious', 1),\n",
       " ('usual', 2),\n",
       " ('cut', 3),\n",
       " ('apothecary', 1),\n",
       " ('particular', 6),\n",
       " ('colour', 7),\n",
       " ('accent', 2),\n",
       " ('emotional', 1),\n",
       " ('rest', 6),\n",
       " ('turn', 8),\n",
       " ('kill', 1),\n",
       " ('killing', 1),\n",
       " ('question', 6),\n",
       " ('told', 11),\n",
       " ('make', 26),\n",
       " ('lose', 1),\n",
       " ('pitching', 1),\n",
       " ('hot', 2),\n",
       " ('women', 3),\n",
       " ('wild', 4),\n",
       " ('harpies', 1),\n",
       " ('hateful', 2),\n",
       " ('middle', 7),\n",
       " ('kind', 6),\n",
       " ('coolness', 2),\n",
       " ('really', 7),\n",
       " ('wishes', 1),\n",
       " ('scene', 2),\n",
       " ('says', 2),\n",
       " ('figure', 7),\n",
       " ('liked', 6),\n",
       " ('money', 5),\n",
       " ('think', 29),\n",
       " ('carried', 7),\n",
       " ('matter', 4),\n",
       " ('gold', 2),\n",
       " ('balance', 4),\n",
       " ('mention', 2),\n",
       " ('points', 3),\n",
       " ('genuine', 3),\n",
       " ('pointing', 3),\n",
       " ('cellar', 3),\n",
       " ('open', 18),\n",
       " ('cash', 1),\n",
       " ('father', 4),\n",
       " ('passed', 8),\n",
       " ('breakfasted', 1),\n",
       " ('bank', 4),\n",
       " ('believe', 8),\n",
       " ('tut', 4),\n",
       " ('feel', 4),\n",
       " ('yes', 18),\n",
       " ('person', 18),\n",
       " ('celebrated', 2),\n",
       " ('blackmail', 1),\n",
       " ('suppose', 10),\n",
       " ('paying', 4),\n",
       " ('nose', 1),\n",
       " ('capers', 1),\n",
       " ('house', 38),\n",
       " ('know', 44),\n",
       " ('far', 17),\n",
       " ('vein', 1),\n",
       " ('suddenly', 12),\n",
       " ('don', 9),\n",
       " ('likely', 1),\n",
       " ('happen', 1),\n",
       " ('square', 10),\n",
       " ('delicacy', 1),\n",
       " ('strongly', 2),\n",
       " ('putting', 2),\n",
       " ('questions', 3),\n",
       " ('partakes', 1),\n",
       " ('style', 1),\n",
       " ('starting', 3),\n",
       " ('stone', 3),\n",
       " ('quietly', 3),\n",
       " ('goes', 4),\n",
       " ('bland', 1),\n",
       " ('bird', 1),\n",
       " ('thought', 39),\n",
       " ('knocked', 4),\n",
       " ('head', 10),\n",
       " ('garden', 6),\n",
       " ('rule', 4),\n",
       " ('looks', 3),\n",
       " ('ask', 13),\n",
       " ('continued', 7),\n",
       " ('looking', 13),\n",
       " ('floor', 9),\n",
       " ('live', 2),\n",
       " ('sure', 13),\n",
       " ('buildings', 1),\n",
       " ('packed', 1),\n",
       " ('ends', 1),\n",
       " ('walked', 7),\n",
       " ('silence', 8),\n",
       " ('harm', 1),\n",
       " ('displeasing', 3),\n",
       " ('right', 15),\n",
       " ('disliked', 1),\n",
       " ('scarce', 8),\n",
       " ('deformed', 1),\n",
       " ('couldn', 1),\n",
       " ('declare', 4),\n",
       " ('consideration', 3),\n",
       " ('surprised', 8),\n",
       " ('correct', 1),\n",
       " ('touch', 4),\n",
       " ('sullenness', 1),\n",
       " ('pedantically', 1),\n",
       " ('ago', 10),\n",
       " ('young', 5),\n",
       " ('refer', 1),\n",
       " ('heart', 11),\n",
       " ('shake', 3),\n",
       " ('bachelor', 1),\n",
       " ('sat', 20),\n",
       " ('relish', 1),\n",
       " ('meal', 1),\n",
       " ('divinity', 1),\n",
       " ('desk', 2),\n",
       " ('neighbouring', 3),\n",
       " ('rang', 2),\n",
       " ('soberly', 1),\n",
       " ('gratefully', 2),\n",
       " ('room', 26),\n",
       " ('opened', 8),\n",
       " ('safe', 7),\n",
       " ('private', 9),\n",
       " ('endorsed', 1),\n",
       " ('envelope', 6),\n",
       " ('clouded', 1),\n",
       " ('brow', 4),\n",
       " ('charge', 7),\n",
       " ('refused', 2),\n",
       " ('assistance', 2),\n",
       " ('making', 2),\n",
       " ('d', 4),\n",
       " ('c', 5),\n",
       " ('l', 3),\n",
       " ('r', 1),\n",
       " ('pass', 3),\n",
       " ('benefactor', 2),\n",
       " ('unexplained', 1),\n",
       " ('period', 2),\n",
       " ('exceeding', 1),\n",
       " ('calendar', 1),\n",
       " ('months', 9),\n",
       " ('step', 7),\n",
       " ('shoes', 1),\n",
       " ('delay', 3),\n",
       " ('free', 7),\n",
       " ('burthen', 2),\n",
       " ('payment', 1),\n",
       " ('sums', 1),\n",
       " ('members', 2),\n",
       " ('eyesore', 1),\n",
       " ('sane', 2),\n",
       " ('immodest', 1),\n",
       " ('knowledge', 5),\n",
       " ('attributes', 1),\n",
       " ('shifting', 1),\n",
       " ('insubstantial', 1),\n",
       " ('leaped', 3),\n",
       " ('replaced', 1),\n",
       " ('obnoxious', 1),\n",
       " ('fear', 14),\n",
       " ('disgrace', 3),\n",
       " ('blew', 1),\n",
       " ('direction', 4),\n",
       " ('cavendish', 3),\n",
       " ('medicine', 4),\n",
       " ('crowding', 1),\n",
       " ('patients', 1),\n",
       " ('welcomed', 2),\n",
       " ('stage', 1),\n",
       " ('healthy', 1),\n",
       " ('dapper', 1),\n",
       " ('faced', 4),\n",
       " ('shock', 2),\n",
       " ('hair', 4),\n",
       " ('boisterous', 1),\n",
       " ('decided', 1),\n",
       " ('chair', 4),\n",
       " ('geniality', 1),\n",
       " ('somewhat', 7),\n",
       " ('mates', 1),\n",
       " ('school', 2),\n",
       " ('college', 1),\n",
       " ('thorough', 2),\n",
       " ('thoroughly', 3),\n",
       " ('rambling', 1),\n",
       " ('preoccupied', 1),\n",
       " ('oldest', 2),\n",
       " ('chuckled', 1),\n",
       " ('unscientific', 1),\n",
       " ('damon', 1),\n",
       " ('pythias', 1),\n",
       " ('spirit', 8),\n",
       " ('temper', 2),\n",
       " ('differed', 3),\n",
       " ('science', 1),\n",
       " ('scientific', 6),\n",
       " ('passions', 3),\n",
       " ('heard', 12),\n",
       " ('dark', 7),\n",
       " ('tossed', 3),\n",
       " ('hours', 9),\n",
       " ('large', 12),\n",
       " ('toiling', 2),\n",
       " ('besieged', 1),\n",
       " ('conveniently', 3),\n",
       " ('near', 11),\n",
       " ('dwelling', 1),\n",
       " ('digging', 2),\n",
       " ('touched', 3),\n",
       " ('imagination', 1),\n",
       " ('engaged', 3),\n",
       " ('gross', 2),\n",
       " ('curtained', 1),\n",
       " ('scroll', 1),\n",
       " ('pictures', 1),\n",
       " ('field', 2),\n",
       " ('swiftly', 5),\n",
       " ('trod', 1),\n",
       " ('regardless', 1),\n",
       " ('screams', 1),\n",
       " ('dreaming', 1),\n",
       " ('plucked', 2),\n",
       " ('apart', 2),\n",
       " ('sleeper', 1),\n",
       " ('lo', 1),\n",
       " ('stand', 2),\n",
       " ('power', 9),\n",
       " ('phases', 1),\n",
       " ('haunted', 1),\n",
       " ('sleeping', 1),\n",
       " ('houses', 4),\n",
       " ('dizziness', 1),\n",
       " ('labyrinths', 1),\n",
       " ('crush', 1),\n",
       " ('leave', 3),\n",
       " ('melted', 4),\n",
       " ('eyes', 20),\n",
       " ('apace', 1),\n",
       " ('curiosity', 9),\n",
       " ('behold', 3),\n",
       " ('mystery', 3),\n",
       " ('lighten', 2),\n",
       " ('roll', 2),\n",
       " ('altogether', 1),\n",
       " ('habit', 4),\n",
       " ('mysterious', 2),\n",
       " ('things', 14),\n",
       " ('examined', 3),\n",
       " ('preference', 1),\n",
       " ('bondage', 2),\n",
       " ('worth', 1),\n",
       " ('seeing', 3),\n",
       " ('mercy', 3),\n",
       " ('enduring', 1),\n",
       " ('hatred', 5),\n",
       " ('haunt', 1),\n",
       " ('noon', 1),\n",
       " ('rewarded', 1),\n",
       " ('fine', 4),\n",
       " ('frost', 2),\n",
       " ('streets', 5),\n",
       " ('ballroom', 1),\n",
       " ('drawing', 3),\n",
       " ('regular', 1),\n",
       " ('pattern', 2),\n",
       " ('shadow', 3),\n",
       " ('low', 6),\n",
       " ('growl', 2),\n",
       " ('round', 8),\n",
       " ('rumour', 1),\n",
       " ('approach', 2),\n",
       " ('minutes', 6),\n",
       " ('footstep', 1),\n",
       " ('nightly', 1),\n",
       " ('accustomed', 3),\n",
       " ('quaint', 3),\n",
       " ('effect', 1),\n",
       " ('footfalls', 1),\n",
       " ('single', 3),\n",
       " ('sharply', 3),\n",
       " ('superstitious', 1),\n",
       " ('success', 1),\n",
       " ('nearer', 5),\n",
       " ('deal', 6),\n",
       " ('dressed', 5),\n",
       " ('distance', 2),\n",
       " ('crossing', 1),\n",
       " ('save', 5),\n",
       " ('stepped', 1),\n",
       " ('shoulder', 2),\n",
       " ('intake', 1),\n",
       " ('answered', 3),\n",
       " ('coolly', 1),\n",
       " ('gaunt', 2),\n",
       " ('meeting', 1),\n",
       " ('admit', 2),\n",
       " ('fronted', 1),\n",
       " ('stared', 4),\n",
       " ('useful', 1),\n",
       " ('_à', 1),\n",
       " ('propos_', 1),\n",
       " ('grunted', 1),\n",
       " ('description', 3),\n",
       " ('echoed', 3),\n",
       " ('anger', 4),\n",
       " ('lied', 1),\n",
       " ('fitting', 1),\n",
       " ('snarled', 1),\n",
       " ('savage', 1),\n",
       " ('laugh', 2),\n",
       " ('quickness', 1),\n",
       " ('unlocked', 2),\n",
       " ('awhile', 3),\n",
       " ('pausing', 1),\n",
       " ('debating', 1),\n",
       " ('class', 1),\n",
       " ('pale', 8),\n",
       " ('impression', 2),\n",
       " ('nameable', 1),\n",
       " ('malformation', 1),\n",
       " ('borne', 2),\n",
       " ('murderous', 1),\n",
       " ('timidity', 1),\n",
       " ('husky', 1),\n",
       " ('unknown', 4),\n",
       " ('disgust', 1),\n",
       " ('regarded', 3),\n",
       " ('perplexed', 1),\n",
       " ('transfigures', 1),\n",
       " ('continent', 1),\n",
       " ('poor', 9),\n",
       " ('harry', 5),\n",
       " ('read', 14),\n",
       " ('new', 14),\n",
       " ('ancient', 1),\n",
       " ('decayed', 1),\n",
       " ('flats', 1),\n",
       " ('sorts', 1),\n",
       " ('conditions', 1),\n",
       " ('architects', 1),\n",
       " ('shady', 1),\n",
       " ('entire', 1),\n",
       " ('wealth', 1),\n",
       " ('plunged', 3),\n",
       " ('stopped', 3),\n",
       " ('comfortable', 2),\n",
       " ('paved', 1),\n",
       " ('flags', 3),\n",
       " ('bright', 3),\n",
       " ('furnished', 4),\n",
       " ('costly', 1),\n",
       " ('cabinets', 2),\n",
       " ('oak', 1),\n",
       " ('fender', 1),\n",
       " ('fancy', 5),\n",
       " ('speak', 8),\n",
       " ('pleasantest', 1),\n",
       " ('heavy', 5),\n",
       " ('felt', 9),\n",
       " ('rare', 3),\n",
       " ('distaste', 2),\n",
       " ('gloom', 1),\n",
       " ('flickering', 1),\n",
       " ('uneasy', 2),\n",
       " ('announce', 1),\n",
       " ('dissecting', 2),\n",
       " ('master', 19),\n",
       " ('repose', 1),\n",
       " ('musingly', 1),\n",
       " ('orders', 5),\n",
       " ('obey', 2),\n",
       " ('_dines_', 1),\n",
       " ('homeward', 1),\n",
       " ('misgives', 1),\n",
       " ('law', 11),\n",
       " ('limitations', 1),\n",
       " ('ghost', 1),\n",
       " ('_pede', 1),\n",
       " ('self', 11),\n",
       " ('love', 3),\n",
       " ('condoned', 1),\n",
       " ('fault', 2),\n",
       " ('scared', 1),\n",
       " ('brooded', 1),\n",
       " ('past', 5),\n",
       " ('groping', 2),\n",
       " ('corners', 1),\n",
       " ('chance', 3),\n",
       " ('iniquity', 2),\n",
       " ('fairly', 2),\n",
       " ('blameless', 1),\n",
       " ('ill', 8),\n",
       " ('raised', 4),\n",
       " ('gratitude', 4),\n",
       " ('avoided', 1),\n",
       " ('secrets', 4),\n",
       " ('compared', 2),\n",
       " ('worst', 3),\n",
       " ('sunshine', 1),\n",
       " ('turns', 1),\n",
       " ('creature', 10),\n",
       " ('thief', 1),\n",
       " ('danger', 6),\n",
       " ('suspects', 1),\n",
       " ('existence', 3),\n",
       " ('impatient', 2),\n",
       " ('inherit', 1),\n",
       " ('shoulders', 4),\n",
       " ('clear', 3),\n",
       " ('excellent', 6),\n",
       " ('dinners', 1),\n",
       " ('cronies', 1),\n",
       " ('judges', 1),\n",
       " ('contrived', 1),\n",
       " ('arrangement', 1),\n",
       " ('befallen', 1),\n",
       " ('scores', 1),\n",
       " ('loved', 1),\n",
       " ('detain', 1),\n",
       " ('hearted', 1),\n",
       " ('loose', 3),\n",
       " ('tongued', 1),\n",
       " ('unobtrusive', 1),\n",
       " ('minds', 1),\n",
       " ('expense', 3),\n",
       " ('exception', 1),\n",
       " ('smooth', 1),\n",
       " ('cast', 6),\n",
       " ('capacity', 1),\n",
       " ('wanting', 2),\n",
       " ('observer', 1),\n",
       " ('gathered', 2),\n",
       " ('topic', 3),\n",
       " ('distasteful', 2),\n",
       " ('gaily', 1),\n",
       " ('unfortunate', 1),\n",
       " ('hide', 3),\n",
       " ('needn', 1),\n",
       " ('frown', 1),\n",
       " ('mean', 6),\n",
       " ('blatant', 1),\n",
       " ('ruthlessly', 1),\n",
       " ('disregarding', 2),\n",
       " ('certainly', 3),\n",
       " ('trifle', 4),\n",
       " ('tell', 12),\n",
       " ('learning', 1),\n",
       " ('lips', 3),\n",
       " ('blackness', 2),\n",
       " ('understand', 4),\n",
       " ('position', 8),\n",
       " ('situated', 1),\n",
       " ('talking', 3),\n",
       " ('trusted', 1),\n",
       " ('downright', 1),\n",
       " ('rid', 1),\n",
       " ('getting', 1),\n",
       " ('sincerely', 1),\n",
       " ('bear', 6),\n",
       " ('pleaded', 1),\n",
       " ('longer', 5),\n",
       " ('year', 5),\n",
       " ('month', 1),\n",
       " ('startled', 1),\n",
       " ('singular', 3),\n",
       " ('notable', 1),\n",
       " ('victim', 7),\n",
       " ('details', 2),\n",
       " ('maid', 9),\n",
       " ('living', 1),\n",
       " ('fog', 7),\n",
       " ('rolled', 3),\n",
       " ('cloudless', 2),\n",
       " ('lane', 3),\n",
       " ('overlooked', 2),\n",
       " ('lit', 1),\n",
       " ('dream', 3),\n",
       " ('streaming', 2),\n",
       " ('tears', 4),\n",
       " ('narrated', 2),\n",
       " ('peace', 3),\n",
       " ('aged', 2),\n",
       " ('advancing', 2),\n",
       " ('meet', 5),\n",
       " ('paid', 8),\n",
       " ('speech', 1),\n",
       " ('bowed', 1),\n",
       " ('politeness', 2),\n",
       " ('watch', 3),\n",
       " ('innocent', 3),\n",
       " ('dislike', 2),\n",
       " ('flame', 1),\n",
       " ('brandishing', 1),\n",
       " ('hurt', 1),\n",
       " ('bounds', 1),\n",
       " ('earth', 2),\n",
       " ('ape', 3),\n",
       " ('trampling', 1),\n",
       " ('hailing', 1),\n",
       " ('storm', 1),\n",
       " ('blows', 1),\n",
       " ('bones', 2),\n",
       " ('audibly', 2),\n",
       " ('shattered', 1),\n",
       " ('sights', 1),\n",
       " ('fainted', 1),\n",
       " ('murderer', 9),\n",
       " ('deed', 2),\n",
       " ('wood', 2),\n",
       " ('stress', 1),\n",
       " ('insensate', 2),\n",
       " ('gutter', 1),\n",
       " ('purse', 1),\n",
       " ('cards', 1),\n",
       " ('papers', 7),\n",
       " ('sealed', 6),\n",
       " ('stamped', 3),\n",
       " ('probably', 2),\n",
       " ('circumstances', 5),\n",
       " ('lip', 1),\n",
       " ('dress', 1),\n",
       " ('grave', 3),\n",
       " ('station', 2),\n",
       " ('exclaimed', 1),\n",
       " ('officer', 3),\n",
       " ('possible', 5),\n",
       " ('professional', 4),\n",
       " ('ambition', 2),\n",
       " ('briefly', 2),\n",
       " ('quailed', 1),\n",
       " ('presented', 1),\n",
       " ('particularly', 3),\n",
       " ('chocolate', 1),\n",
       " ('continually', 3),\n",
       " ('charging', 1),\n",
       " ('embattled', 1),\n",
       " ('vapours', 1),\n",
       " ('beheld', 2),\n",
       " ('marvelous', 1),\n",
       " ('degrees', 1),\n",
       " ('conflagration', 1),\n",
       " ('haggard', 1),\n",
       " ('shaft', 1),\n",
       " ('daylight', 3),\n",
       " ('swirling', 1),\n",
       " ('wreaths', 1),\n",
       " ('dismal', 2),\n",
       " ('glimpses', 1),\n",
       " ('slatternly', 1),\n",
       " ('passengers', 3),\n",
       " ('afresh', 1),\n",
       " ('mournful', 1),\n",
       " ('reinvasion', 1),\n",
       " ('nightmare', 1),\n",
       " ('glanced', 1),\n",
       " ('conscious', 9),\n",
       " ('terror', 13),\n",
       " ('officers', 1),\n",
       " ('assail', 1),\n",
       " ('palace', 1),\n",
       " ('eating', 1),\n",
       " ('numbers', 1),\n",
       " ('twopenny', 1),\n",
       " ('ragged', 1),\n",
       " ('huddled', 2),\n",
       " ('different', 5),\n",
       " ('glass', 15),\n",
       " ('settled', 1),\n",
       " ('umber', 1),\n",
       " ('heir', 2),\n",
       " ('ivory', 1),\n",
       " ('silvery', 1),\n",
       " ('evil', 17),\n",
       " ('smoothed', 1),\n",
       " ('hypocrisy', 1),\n",
       " ('late', 9),\n",
       " ('habits', 1),\n",
       " ('absent', 1),\n",
       " ('rooms', 4),\n",
       " ('impossible', 4),\n",
       " ('inspector', 3),\n",
       " ('newcomen', 1),\n",
       " ('scotland', 1),\n",
       " ('yard', 5),\n",
       " ('flash', 1),\n",
       " ('trouble', 2),\n",
       " ('glances', 1),\n",
       " ('extent', 2),\n",
       " ('filled', 5),\n",
       " ('silver', 1),\n",
       " ('napery', 1),\n",
       " ('elegant', 1),\n",
       " ('hung', 3),\n",
       " ('walls', 1),\n",
       " ('gift', 2),\n",
       " ('carpets', 1),\n",
       " ('agreeable', 1),\n",
       " ('lock', 5),\n",
       " ('drawers', 1),\n",
       " ('pile', 1),\n",
       " ('grey', 1),\n",
       " ('embers', 1),\n",
       " ('butt', 1),\n",
       " ('green', 2),\n",
       " ('book', 6),\n",
       " ('action', 4),\n",
       " ('clinched', 1),\n",
       " ('suspicions', 3),\n",
       " ('delighted', 2),\n",
       " ('visit', 4),\n",
       " ('thousand', 3),\n",
       " ('lost', 7),\n",
       " ('accomplishment', 1),\n",
       " ('familiars', 1),\n",
       " ('traced', 1),\n",
       " ('photographed', 1),\n",
       " ('widely', 1),\n",
       " ('haunting', 1),\n",
       " ('fugitive', 1),\n",
       " ('impressed', 1),\n",
       " ('afternoon', 1),\n",
       " ('admitted', 2),\n",
       " ('offices', 2),\n",
       " ('bought', 1),\n",
       " ('heirs', 1),\n",
       " ('anatomical', 2),\n",
       " ('changed', 8),\n",
       " ('destination', 1),\n",
       " ('quarters', 1),\n",
       " ('straw', 3),\n",
       " ('falling', 1),\n",
       " ('foggy', 1),\n",
       " ('flight', 2),\n",
       " ('covered', 5),\n",
       " ('fitted', 1),\n",
       " ('barred', 1),\n",
       " ('grate', 2),\n",
       " ('lie', 3),\n",
       " ('thickly', 2),\n",
       " ('bade', 2),\n",
       " ('news', 4),\n",
       " ('crying', 4),\n",
       " ('bind', 1),\n",
       " ('honour', 5),\n",
       " ('feverish', 1),\n",
       " ('trial', 1),\n",
       " ('appear', 4),\n",
       " ('share', 3),\n",
       " ('advise', 1),\n",
       " ('loss', 2),\n",
       " ('wisely', 1),\n",
       " ('exposed', 2),\n",
       " ('selfishness', 2),\n",
       " ('relieved', 3),\n",
       " ('labour', 1),\n",
       " ('means', 7),\n",
       " ('escape', 5),\n",
       " ('placed', 1),\n",
       " ('intimacy', 1),\n",
       " ('postmark', 1),\n",
       " ('handed', 3),\n",
       " ('consider', 4),\n",
       " ('mouth', 2),\n",
       " ('tight', 1),\n",
       " ('solemnly', 3),\n",
       " ('bye', 1),\n",
       " ('positive', 1),\n",
       " ('handled', 1),\n",
       " ('caution', 1),\n",
       " ('hoarse', 1),\n",
       " ('edition', 2),\n",
       " ('p', 2),\n",
       " ('funeral', 2),\n",
       " ('lest', 2),\n",
       " ('sucked', 1),\n",
       " ('eddy', 1),\n",
       " ('ticklish', 1),\n",
       " ('decision', 2),\n",
       " ('longing', 1),\n",
       " ('fished', 1),\n",
       " ('guest', 10),\n",
       " ('dwelt', 1),\n",
       " ('slept', 6),\n",
       " ('sound', 3),\n",
       " ('mighty', 1),\n",
       " ('acids', 1),\n",
       " ('softened', 1),\n",
       " ('grows', 1),\n",
       " ('stained', 2),\n",
       " ('vineyards', 1),\n",
       " ('fewer', 1),\n",
       " ('failed', 2),\n",
       " ('draw', 2),\n",
       " ('student', 1),\n",
       " ('critic', 1),\n",
       " ('handwriting', 2),\n",
       " ('dropping', 2),\n",
       " ('shape', 5),\n",
       " ('sad', 2),\n",
       " ('public', 8),\n",
       " ('views', 3),\n",
       " ('autograph', 2),\n",
       " ('brightened', 1),\n",
       " ('passion', 2),\n",
       " ('accounts', 1),\n",
       " ('entered', 4),\n",
       " ('writing', 6),\n",
       " ('alongside', 1),\n",
       " ('sedulously', 1),\n",
       " ('interesting', 1),\n",
       " ('compare', 1),\n",
       " ('identical', 1),\n",
       " ('sloped', 1),\n",
       " ('offered', 1),\n",
       " ('reward', 1),\n",
       " ('resented', 2),\n",
       " ('injury', 1),\n",
       " ('existed', 1),\n",
       " ('unearthed', 1),\n",
       " ('disreputable', 1),\n",
       " ('violent', 1),\n",
       " ('associates', 1),\n",
       " ('career', 2),\n",
       " ('whereabouts', 1),\n",
       " ('gradually', 2),\n",
       " ('hotness', 1),\n",
       " ('withdrawn', 2),\n",
       " ('seclusion', 2),\n",
       " ('entertainer', 1),\n",
       " ('charities', 2),\n",
       " ('religion', 2),\n",
       " ('inward', 1),\n",
       " ('consciousness', 5),\n",
       " ('service', 4),\n",
       " ('january', 3),\n",
       " ('host', 1),\n",
       " ('days', 13),\n",
       " ('trio', 1),\n",
       " ('inseparable', 1),\n",
       " ('dine', 1),\n",
       " ('sixth', 1),\n",
       " ('betook', 1),\n",
       " ('warrant', 1),\n",
       " ('tokens', 1),\n",
       " ('quality', 2),\n",
       " ('seated', 2),\n",
       " ('unlikely', 1),\n",
       " ('tempted', 5),\n",
       " ('firmness', 1),\n",
       " ('doomed', 2),\n",
       " ('weeks', 1),\n",
       " ('spare', 2),\n",
       " ('allusion', 1),\n",
       " ('regard', 1),\n",
       " ('accursed', 2),\n",
       " ('cause', 6),\n",
       " ('unhappy', 5),\n",
       " ('pathetically', 1),\n",
       " ('darkly', 1),\n",
       " ('quarrel', 1),\n",
       " ('blame', 2),\n",
       " ('extreme', 2),\n",
       " ('suffer', 4),\n",
       " ('sinners', 1),\n",
       " ('terrors', 2),\n",
       " ('amazed', 2),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "book_43_counts_filtered = book_43_counts.filter(lambda x: x[0] not in stop_words)\n",
    "\n",
    "book_43_counts_filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b3a80",
   "metadata": {},
   "source": [
    "### Q11\n",
    "\n",
    "* How many words are left in `book_43_counts_filtered` after removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5bdf083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034 are left\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "print(f\"{book_43_counts_filtered.count()} are left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffd755",
   "metadata": {},
   "source": [
    "### Q12 \n",
    "\n",
    "* Create a function called *process_RDD* that combines the relevant steps you proposed above to make it convenient to apply them to the remaining four books. Your function should accept an input text file path and:\n",
    " * Reads in the file as a textRDD\n",
    " * Cleans and splits the line using `clean_split_line`\n",
    " * Filters out the stop words\n",
    " * Returns a word count RDD where each item is a tuple of words and its count.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff5d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "def process_RDD(file_path):\n",
    "    # read file\n",
    "    book = sc.textFile(file_path)\n",
    "    # clean and split lines\n",
    "    words = book.flatMap(clean_split_line)\n",
    "    # count the words\n",
    "    book_counts = words.map(lambda x: (x.lower(), 1)).reduceByKey(add)\n",
    "    # filter out stop words\n",
    "    book_counts_filtered = book_counts.filter(lambda x: x[0] not in stop_words)\n",
    "\n",
    "    return book_counts_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59c9c5",
   "metadata": {},
   "source": [
    "### Q13 \n",
    "\n",
    "Apply the function `process_RDD` to `book_84`, `book_398` and `book_3296` and save the results to variables `book_84_counts_filtered`, `book_398_counts_filtered` and `book_3296_counts_filtered` respectively. How many distinct words does each book contain after filtering the stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a24f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_84: 7016 words\n",
      "book_398: 2421 words\n",
      "book_3296: 7293 words\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "book_84_counts_filtered = process_RDD('data/84.txt')\n",
    "book_398_counts_filtered = process_RDD('data/398.txt')\n",
    "book_3296_counts_filtered = process_RDD('data/3296.txt')\n",
    "\n",
    "print(f\"book_84: {book_84_counts_filtered.count()} words\")\n",
    "print(f\"book_398: {book_398_counts_filtered.count()} words\")\n",
    "print(f\"book_3296: {book_3296_counts_filtered.count()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9680f",
   "metadata": {},
   "source": [
    "### Q14 \n",
    "\n",
    "We discussed how to evaluate similarity between two texts using the number of words they share. We hypothesized that books that are similar should have more words in common than books that are dissimilar. If that holds, `book_398` and `book_3296`, which both pertain to religion, will have more words in common than, say, `book_84` and `book_398`. Test this hypothesis by writing code that compares and prints the number of words shared between `book_398` and `book_3296` and then between `book_84` and `book_398`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef9fa114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_398 and book_3296: 1790 words in common\n",
      "book_84 and book_398: 1691 words in common\n"
     ]
    }
   ],
   "source": [
    "book_398_words = book_398_counts_filtered.keys().collect()\n",
    "\n",
    "book3296_and_book398 = book_3296_counts_filtered.filter(lambda x: x[0] in book_398_words).count()\n",
    "book84_and_book398 = book_84_counts_filtered.filter(lambda x: x[0] in book_398_words).count()\n",
    "\n",
    "print(f\"book_398 and book_3296: {book3296_and_book398} words in common\")\n",
    "print(f\"book_84 and book_398: {book84_and_book398} words in common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9f326",
   "metadata": {},
   "source": [
    "### Q15\n",
    "\n",
    "* Based on the above, do you think counting the number of shared words is a good idea as a distance metric for evaluating topic similarity? Justify your answer?\n",
    "* Hint: What do *book_84* and *book_3296* have in common? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f53226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_84 and book_3296: 3608 words in common\n"
     ]
    }
   ],
   "source": [
    "book_84_words = book_84_counts_filtered.keys().collect()\n",
    "\n",
    "book84_and_book3296 = book_3296_counts_filtered.filter(lambda x: x[0] in book_84_words).count()\n",
    "\n",
    "print(f\"book_84 and book_3296: {book84_and_book3296} words in common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587ae14",
   "metadata": {},
   "source": [
    "#### Write your answer here\n",
    "\n",
    "From the results above, book_84 and book_3296 have more words in common compared to book_398 and book_3296, which means that it is not a good idea to compare similarity by using the amount of shared words. \n",
    "\n",
    "This is because book_398 and book_3296 should have a higher count of shared words since they are similar books, with both pertaining to religion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8f857",
   "metadata": {},
   "source": [
    "## Part II \n",
    "\n",
    "Another approach to estimating similarity consists of computing the Euclidean distance across a set of words. For example, suppose we have 3 documents A, B and C with the following counts for the words `evolution`, `DNA`, `biology` and `finance`. \n",
    "\n",
    "```python \n",
    "A = [4, 9, 6, 8]\n",
    "B = [3, 7, 7, 10]\n",
    "C = [15, 10, 1, 1]\n",
    "```\n",
    "Although all documents contain exactly the four words, the number of times these words appear in each book may be indicative of thier topic. For example, documents `A` and `B` are more likely to be business related since they contain the word `finance` more frequently (8 and 10 times respectively). Document `C` may be a technical document since it focuses on more technical words (`evolution` and `DNA`) and less on the words `finance`.\n",
    "\n",
    "The Euclidean distance, which can be computed using the `scikit` snippet below, is more indicative of topic-relatedness between the two documents.\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import euclidean \n",
    "print(f\"The Euclidean distance between A and B is: {euclidean(A, B)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between A and C is: {euclidean(A, C)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between B and C is: {euclidean(B, C)}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7838f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between A and B is: 3.1622776601683795\n",
      "The Euclidean distance between A and C is: 14.0\n",
      "The Euclidean distance between B and C is: 16.431676725154983\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "from scipy.spatial.distance import euclidean \n",
    "\n",
    "A = [4, 9, 6, 8]\n",
    "B = [3, 7, 7, 10]\n",
    "C = [15, 10, 1, 1]\n",
    "\n",
    "print(f\"The Euclidean distance between A and B is: {euclidean(A, B)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between A and C is: {euclidean(A, C)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between B and C is: {euclidean(B, C)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdc35e",
   "metadata": {},
   "source": [
    "### Q16\n",
    "\n",
    "To calculate the Euclidean distance, we must first identify the set of words by which we will compare the documents. Here, we will explore the words that are common to all 4 documents. We will store the data in a matrix called `counts_matrix`.\n",
    "\n",
    "Start by finding the words that are common to all four documents after stop-word filtering and store the counts for each word in a column of `counts_matrix`. \n",
    "\n",
    "To take the previous example, you can generate an emtpy matrix with 3 lines (documents `A`, `B` and `C`) and 4 columns (words `evolution`, `DNA`, `biology` and `finance`) using the following code.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "counts_matrix = np.zeros([3,4])\n",
    "```\n",
    "\n",
    "After generting the counts, you can fill the counts for a document, say `A`, using the following code:\n",
    "\n",
    "```python\n",
    "counts_matrix[0, :] = [4, 9, 6, 8] \n",
    "```\n",
    "* Other than for building `counts_matrix` you should exclusively use operations or actions on the `RDD` to answer this question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cae946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  3.  2. ...  5.  3.  1.]\n",
      " [ 1.  9.  4. ... 30.  9.  1.]\n",
      " [ 1.  5. 16. ...  1.  2.  1.]\n",
      " [16. 36.  3. ... 15.  4.  1.]]\n"
     ]
    }
   ],
   "source": [
    "### Write your code \n",
    "import numpy as np\n",
    "\n",
    "# join all books to filter out non-common words between all 4 books.\n",
    "joined_books = book_43_counts_filtered.join(book_84_counts_filtered).join(book_398_counts_filtered).join(book_3296_counts_filtered)\n",
    "joined_books_words = joined_books.keys().collect()\n",
    "\n",
    "# take only the count of words\n",
    "# filter -> sort alphabetically -> take only the count and remove the word\n",
    "book_43_count_only = book_43_counts_filtered.filter(lambda x: x[0] in joined_books_words).sortByKey(True).flatMap(lambda x: [x[1]])\n",
    "book_84_count_only = book_84_counts_filtered.filter(lambda x: x[0] in joined_books_words).sortByKey(True).flatMap(lambda x: [x[1]])\n",
    "book_398_count_only = book_398_counts_filtered.filter(lambda x: x[0] in joined_books_words).sortByKey(True).flatMap(lambda x: [x[1]])\n",
    "book_3296_count_only = book_3296_counts_filtered.filter(lambda x: x[0] in joined_books_words).sortByKey(True).flatMap(lambda x: [x[1]])\n",
    "\n",
    "\n",
    "# create the book matrix\n",
    "counts_matrix = np.zeros([4, book_43_count_only.count()])\n",
    "\n",
    "counts_matrix[0, :] = book_43_count_only.collect()\n",
    "counts_matrix[1, :] = book_84_count_only.collect()\n",
    "counts_matrix[2, :] = book_398_count_only.collect()\n",
    "counts_matrix[3, :] = book_3296_count_only.collect()\n",
    "\n",
    "print(counts_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bcdd8",
   "metadata": {},
   "source": [
    "### Q17\n",
    "\n",
    "Compute the Euclidean distance between `book_398` and `book_3296`, which both talk about religion and `book_84` and `book_398`. What do you conclude about using the Euclidean distance for evaluating topic relatedness across documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6a81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between book_398 and book_3296 is: 1156.6628722320086\n",
      "The Euclidean distance between book_84 and book_398 is: 751.6688100486809\n",
      "The Euclidean distance between book_84 and book_3296 is: 1171.2493329773981\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "print(f\"The Euclidean distance between book_398 and book_3296 is: {euclidean(counts_matrix[2], counts_matrix[3])}\")\n",
    "print(f\"The Euclidean distance between book_84 and book_398 is: {euclidean(counts_matrix[1], counts_matrix[2])}\")\n",
    "print(f\"The Euclidean distance between book_84 and book_3296 is: {euclidean(counts_matrix[1], counts_matrix[3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89981b46",
   "metadata": {},
   "source": [
    "#### Write your justification here\n",
    "\n",
    "Although the euclidean distance shows better result when it comes to comparing book_398 with book_3296 and book_84 with book_398, it is still not a good way to evaluate topic relatedness across documents. This is because the euclidean distance between book_84 and book_3296 is higher than the euclidean distance between book_84 and book_398. \n",
    "\n",
    "This means that based on the euclidean distance, book_84 is more similar with book_3296 than book_398 when that should not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e54e6",
   "metadata": {},
   "source": [
    "### Q18\n",
    "\n",
    "Bonus question (5 points): Can you think of a few things we could do to improve similarity between documents that pertain to the same topic. Jutify your answer without given codem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c6de1",
   "metadata": {},
   "source": [
    "#### Write your answer here\n",
    "\n",
    "A way to improve similarity between documents is to not only look at word frequency, but also look at the context or the semantics of the way that the words are being used. This makes it so that it does not fall into the trap of thinking that two books are similar because of the word frequency such as the case with book_84 being compared to book_3296 and book_398.\n",
    "\n",
    "As an example, two phrases such as \"man bites dog\" and \"dog bites man\" are two different phrases contextually but have the same word frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1d74",
   "metadata": {},
   "source": [
    "## Part III\n",
    "\n",
    "In this part we will build some basic analytics for a dataset consisting of flight arrival and departure details for all commercial flights within the USA in one month. While this dataset can be managed using Pandas (<1M records), scaling to a yearly or longer timeframe will greatly benefit from using a distributed computing framework such as `Spark`.\n",
    "\n",
    "Here, you should use exclusively `SparkDatFrames. \n",
    "\n",
    "We want to analyze this dataset to better schedule trips.  For example:\n",
    " * Avoid airlines carriers that are most often associated with delays.\n",
    " * Avoid departure days where delays are most frequent.\n",
    " * Avoid airports which are associated with delays or long taxxying time.\n",
    "* etc.\n",
    " \n",
    "\n",
    "The information about the fields contained in the data file can be found [here](https://dataverse.harvard.edu/dataset.xhtml;jsessionid=0414e25969eccd0e88ae4d64fa0b?persistentId=doi%3A10.7910%2FDVN%2FHG7NV7&version=&q=&fileTypeGroupFacet=&fileTag=%221.+Documentation%22&fileSortField=date&fileSortOrder=desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed25bf2",
   "metadata": {},
   "source": [
    "### Q19\n",
    "\n",
    "Load the file `flight_info.csv` into a spark `DataFrame` called `fight_info`.\n",
    "\n",
    "  * Note that you will need to create a sparkSession prior to loading the data\n",
    "  \n",
    "* How many entries does the file contain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ad5e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 70:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450017 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "ss = SparkSession(sc)\n",
    "flight_info = ss.read.csv('data/flight_info.csv', header=True)\n",
    "\n",
    "print(f\"{flight_info.count()} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1e895",
   "metadata": {},
   "source": [
    "### Q20\n",
    "\n",
    "Use `pySpark-SQL` or `pandas-like syntax to compute the airlines represented in this dataset\n",
    "The airline information is stored in a field called UniqueCarrier\n",
    "* UniqueCarrier: Represents the unique carrier code (ex.AA = American Airlines) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73fd7b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 73:>                                                         (0 + 4) / 4]\r",
      "\r",
      "[Stage 73:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|UniqueCarrier|\n",
      "+-------------+\n",
      "|           UA|\n",
      "|           NK|\n",
      "|           AA|\n",
      "|           EV|\n",
      "|           B6|\n",
      "|           DL|\n",
      "|           OO|\n",
      "|           F9|\n",
      "|           HA|\n",
      "|           WN|\n",
      "|           AS|\n",
      "|           VX|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 73:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "flight_info.select(\"UniqueCarrier\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5c0ae",
   "metadata": {},
   "source": [
    "### Q21\n",
    "\n",
    "The data file contains various other fields, two of which are useful for answering the next question.\n",
    "\n",
    "* CRSDepTime: Represents the scheduled departure time\n",
    "* DepTime: Represents the actual departure time\n",
    "\n",
    "Compute the number of flights delayed per each carried code represented in this dataset. Sort the data by decreasing order of delays.\n",
    "  * A delay is observed when `DepTime` > `CRSDepTime`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c72cff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|UniqueCarrier|count|\n",
      "+-------------+-----+\n",
      "|           WN|54557|\n",
      "|           DL|28962|\n",
      "|           AA|26291|\n",
      "|           UA|19594|\n",
      "|           OO|17924|\n",
      "|           EV|12340|\n",
      "|           B6|10406|\n",
      "|           AS| 4966|\n",
      "|           NK| 4435|\n",
      "|           F9| 3181|\n",
      "|           VX| 2871|\n",
      "|           HA| 2265|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "(flight_info\n",
    ".filter(flight_info[\"DepTime\"] > flight_info[\"CRSDepTime\"])\n",
    ".groupby(\"UniqueCarrier\")\n",
    ".count()\n",
    ".sort(\"count\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4414266",
   "metadata": {},
   "source": [
    "### Q22\n",
    "\n",
    " Use the file `airlines.csv` to find the the complete name of the airline. Here, you are required to load the file as a pyspark DataFrame; call it `airlines_info`, and repeat the query above while including the `flights.csv `file in your query (requires doing a `join`) so that you can also display the full name of the carrier (second column). \n",
    "\n",
    "The result will look (approximately) like:\n",
    "\n",
    "```\n",
    "[Row(UniqueCarrier='WN', first(_c1)='Southwest Airlines', count=SOME_count),\n",
    " Row(UniqueCarrier='DL', first(_c1)='Delta Air Lines', count=SOME_count),\n",
    " Row(UniqueCarrier='AA', first(_c1)='American Airlines', count=SOME_count),\n",
    " ...\n",
    " ]\n",
    "```\n",
    "\n",
    "The carrier code in the `airlines.csv` file is provided in the 4th (1-based) column\n",
    "\n",
    "Note that the file `airlines.csv` does not have column header. Hence, you need to print one line of your dataset to see what names Spark gave to the columns. Use the name provided by Spark in your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30cea22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+----+----+---------------+--------------+---+\n",
      "|_c0|                 _c1|_c2| _c3| _c4|            _c5|           _c6|_c7|\n",
      "+---+--------------------+---+----+----+---------------+--------------+---+\n",
      "|  1|      Private flight| \\N|   -| N/A|           null|          null|  Y|\n",
      "|  2|         135 Airways| \\N|null| GNL|        GENERAL| United States|  N|\n",
      "|  3|       1Time Airline| \\N|  1T| RNX|        NEXTIME|  South Africa|  Y|\n",
      "|  4|2 Sqn No 1 Elemen...| \\N|null| WYT|           null|United Kingdom|  N|\n",
      "|  5|     213 Flight Unit| \\N|null| TFU|           null|        Russia|  N|\n",
      "|  6|223 Flight Unit S...| \\N|null| CHD| CHKALOVSK-AVIA|        Russia|  N|\n",
      "|  7|   224th Flight Unit| \\N|null| TTF|     CARGO UNIT|        Russia|  N|\n",
      "|  8|         247 Jet Ltd| \\N|null| TWF|   CLOUD RUNNER|United Kingdom|  N|\n",
      "|  9|         3D Aviation| \\N|null| SEC|        SECUREX| United States|  N|\n",
      "| 10|         40-Mile Air| \\N|  Q5| MLA|       MILE-AIR| United States|  Y|\n",
      "| 11|              4D Air| \\N|null| QRT|        QUARTET|      Thailand|  N|\n",
      "| 12|611897 Alberta Li...| \\N|null| THD|          DONUT|        Canada|  N|\n",
      "| 13|    Ansett Australia| \\N|  AN| AAA|         ANSETT|     Australia|  Y|\n",
      "| 14|Abacus International| \\N|  1B|null|           null|     Singapore|  Y|\n",
      "| 15|     Abelag Aviation| \\N|  W9| AAB|            ABG|       Belgium|  N|\n",
      "| 16|      Army Air Corps| \\N|null| AAC|        ARMYAIR|United Kingdom|  N|\n",
      "| 17|Aero Aviation Cen...| \\N|null| AAD|        SUNRISE|        Canada|  N|\n",
      "| 18|Aero Servicios Ej...| \\N|null| SII|         ASEISA|        Mexico|  N|\n",
      "| 19|         Aero Biniza| \\N|null| BZS|         BINIZA|        Mexico|  N|\n",
      "| 20|       Aero Albatros| \\N|null| ABM|ALBATROS ESPANA|         Spain|  N|\n",
      "+---+--------------------+---+----+----+---------------+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "airlines_info = ss.read.csv(\"data/airlines.csv\")\n",
    "\n",
    "airlines_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d29e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|UniqueCarrier|          first(_c1)|count|\n",
      "+-------------+--------------------+-----+\n",
      "|           WN|  Southwest Airlines|54557|\n",
      "|           DL|     Delta Air Lines|28962|\n",
      "|           AA|   American Airlines|26291|\n",
      "|           UA|     United Airlines|19594|\n",
      "|           OO|             SkyWest|17924|\n",
      "|           EV|Atlantic Southeas...|12340|\n",
      "|           B6|     JetBlue Airways|10406|\n",
      "|           AS|     Alaska Airlines| 4966|\n",
      "|           NK|     Spirit Airlines| 4435|\n",
      "|           F9|   Frontier Airlines| 3181|\n",
      "|           VX|      Virgin America| 2871|\n",
      "|           HA|   Hawaiian Airlines| 2265|\n",
      "+-------------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(airlines_info\n",
    " .join(flight_info, airlines_info[\"_c3\"] == flight_info[\"UniqueCarrier\"])\n",
    " .filter(\"DepTime > CRSDepTime\")\n",
    " .groupby(\"UniqueCarrier\")\n",
    " .agg(F.first(\"_c1\"), F.count(\"UniqueCarrier\").alias(\"count\"))\n",
    " .sort(\"count\", ascending=False)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0123a8",
   "metadata": {},
   "source": [
    "### Q23\n",
    "\n",
    "Compute the number of delays per company per day. The day is encoded as an integer in the column `DayOfWeek` in `fight_info`. You can display the day as an integer or map it into a string name of the week.\n",
    "Sort the data by airline code (UniqueCarrier) and by increasing values of DayOfWeek\n",
    "\n",
    "\n",
    "You results should look like the following\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9971baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 85:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------+\n",
      "|UniqueCarrier|DayOfWeek|DelayedFlights|\n",
      "+-------------+---------+--------------+\n",
      "|           AA|        1|          5117|\n",
      "|           AA|        2|          3688|\n",
      "|           AA|        3|          2941|\n",
      "|           AA|        4|          3575|\n",
      "|           AA|        5|          3525|\n",
      "|           AA|        6|          2800|\n",
      "|           AA|        7|          4645|\n",
      "|           AS|        1|           872|\n",
      "|           AS|        2|           669|\n",
      "|           AS|        3|           618|\n",
      "|           AS|        4|           728|\n",
      "|           AS|        5|           630|\n",
      "|           AS|        6|           517|\n",
      "|           AS|        7|           932|\n",
      "|           B6|        1|          1892|\n",
      "|           B6|        2|          1787|\n",
      "|           B6|        3|          1253|\n",
      "|           B6|        4|          1254|\n",
      "|           B6|        5|          1309|\n",
      "|           B6|        6|          1027|\n",
      "+-------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 85:==============>                                           (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "delayed_flights = (flight_info\n",
    " .filter(\"DepTime > CRSDepTime\")\n",
    " .groupby(\"UniqueCarrier\", \"DayOfWeek\")\n",
    " .agg(F.count(\"UniqueCarrier\").alias(\"DelayedFlights\"))\n",
    " .sort(\"UniqueCarrier\", \"DayOfWeek\", ascending=True))\n",
    "\n",
    "delayed_flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d986e",
   "metadata": {},
   "source": [
    "### Q24  \n",
    "\n",
    "Counting the number of delayed flights per airline is misleading, as airlines with more flights are more likley to have delays than companies with substantially fiewer flights. \n",
    "\n",
    "Repeat the same query above but, for each carrier, normalize the counts of delays by the total number of flights for that carrier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9986003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flights = (flight_info\n",
    " .groupby(\"UniqueCarrier\", \"DayOfWeek\")\n",
    " .agg(F.count(\"UniqueCarrier\").alias(\"TotalFlights\"))\n",
    " .sort(\"UniqueCarrier\", \"DayOfWeek\", ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ae9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------------+\n",
      "|UniqueCarrier|DayOfWeek|   NormalizedDelays|\n",
      "+-------------+---------+-------------------+\n",
      "|           AA|        1| 0.4206329634196465|\n",
      "|           AA|        2|0.31950099627479855|\n",
      "|           AA|        3|0.30285243538255585|\n",
      "|           AA|        4|0.36133009904992924|\n",
      "|           AA|        5|0.35448511665325827|\n",
      "|           AA|        6| 0.3415883859948762|\n",
      "|           AA|        7| 0.3977564651481418|\n",
      "|           AS|        1| 0.3553382233088835|\n",
      "|           AS|        2|0.29099608525445847|\n",
      "|           AS|        3| 0.3333333333333333|\n",
      "|           AS|        4| 0.3731419784725782|\n",
      "|           AS|        5|0.32142857142857145|\n",
      "|           AS|        6| 0.2875417130144605|\n",
      "|           AS|        7|0.38914405010438413|\n",
      "|           B6|        1|0.45811138014527847|\n",
      "|           B6|        2|0.44899497487437184|\n",
      "|           B6|        3|0.39601769911504425|\n",
      "|           B6|        4|0.38278388278388276|\n",
      "|           B6|        5|0.40055079559363527|\n",
      "|           B6|        6| 0.3583391486392184|\n",
      "+-------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 13.5 ms, sys: 949 µs, total: 14.5 ms\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(delayed_flights\n",
    " .join(total_flights, [\"UniqueCarrier\", \"DayOfWeek\"])\n",
    " .withColumn(\"NormalizedDelays\", delayed_flights[\"DelayedFlights\"] / total_flights[\"TotalFlights\"])\n",
    " .sort(\"UniqueCarrier\", \"DayOfWeek\", ascending=True)\n",
    " .select(\"UniqueCarrier\", \"DayOfWeek\", \"NormalizedDelays\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e91af2",
   "metadata": {},
   "source": [
    "### Q25 \n",
    "\n",
    "Time the query above. How long did it take to run. \n",
    "  * Make sure you run the code a few times and compute the average run time.\n",
    "  * The above should be easy to implement if you use the correct Jupyter Notebook `magic` function\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df43da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took on average 1.3139999999999998s to run\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "# in seconds\n",
    "times = (1.38, 1.27, 1.24, 1.38, 1.3)\n",
    "\n",
    "print(f\"It took on average {sum(times) / len(times)}s to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83b71a",
   "metadata": {},
   "source": [
    "### Q26 \n",
    "\n",
    "Use one of the techniques covered in class to accelerate this query. Time your query to see by how much the run time was improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ca673ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[UniqueCarrier: string, DayOfWeek: string, TotalFlights: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delayed_flights.cache()\n",
    "total_flights.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e79a6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------------+\n",
      "|UniqueCarrier|DayOfWeek|   NormalizedDelays|\n",
      "+-------------+---------+-------------------+\n",
      "|           AA|        1| 0.4206329634196465|\n",
      "|           AA|        2|0.31950099627479855|\n",
      "|           AA|        3|0.30285243538255585|\n",
      "|           AA|        4|0.36133009904992924|\n",
      "|           AA|        5|0.35448511665325827|\n",
      "|           AA|        6| 0.3415883859948762|\n",
      "|           AA|        7| 0.3977564651481418|\n",
      "|           AS|        1| 0.3553382233088835|\n",
      "|           AS|        2|0.29099608525445847|\n",
      "|           AS|        3| 0.3333333333333333|\n",
      "|           AS|        4| 0.3731419784725782|\n",
      "|           AS|        5|0.32142857142857145|\n",
      "|           AS|        6| 0.2875417130144605|\n",
      "|           AS|        7|0.38914405010438413|\n",
      "|           B6|        1|0.45811138014527847|\n",
      "|           B6|        2|0.44899497487437184|\n",
      "|           B6|        3|0.39601769911504425|\n",
      "|           B6|        4|0.38278388278388276|\n",
      "|           B6|        5|0.40055079559363527|\n",
      "|           B6|        6| 0.3583391486392184|\n",
      "+-------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 7.07 ms, sys: 15.4 ms, total: 22.5 ms\n",
      "Wall time: 921 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(delayed_flights\n",
    " .join(total_flights.hint(\"broadcast\"), [\"UniqueCarrier\", \"DayOfWeek\"])\n",
    " .withColumn(\"NormalizedDelays\", delayed_flights[\"DelayedFlights\"] / total_flights[\"TotalFlights\"])\n",
    " .sort(\"UniqueCarrier\", \"DayOfWeek\", ascending=True)\n",
    " .select(\"UniqueCarrier\", \"DayOfWeek\", \"NormalizedDelays\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "575f0ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took on average 0.56s to run\n"
     ]
    }
   ],
   "source": [
    "# in seconds\n",
    "times = (0.563, 0.598, 0.567, 0.537, 0.535)\n",
    "\n",
    "print(f\"It took on average {sum(times) / len(times)}s to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0204d",
   "metadata": {},
   "source": [
    "### Q27 \n",
    "\n",
    "Is the departure delay (i.e., DepTime - CRSDepTime) predictive of the arrival delay (ArrTime > CRSArrTime)?\n",
    "Use an approach of your choice (e.g. `skelearn` which we covered in class or `Spark`) to model as a linear regression the arrival delay as a function of the departure delay. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89f563d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9adc785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>-30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440741</th>\n",
       "      <td>-43.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440742</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440743</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440744</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440745</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x     y\n",
       "0       -3.0 -14.0\n",
       "1       -5.0 -35.0\n",
       "2       -8.0 -30.0\n",
       "3       37.0  73.0\n",
       "4       13.0   2.0\n",
       "...      ...   ...\n",
       "440741 -43.0   3.0\n",
       "440742  -6.0   2.0\n",
       "440743  -3.0 -20.0\n",
       "440744  -4.0 -57.0\n",
       "440745  -5.0 -31.0\n",
       "\n",
       "[440746 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "delays = (flight_info\n",
    ".filter(\"Cancelled = 0 AND DepTime IS NOT NULL AND ArrTime IS NOT NULL\")\n",
    ".select((F.col(\"DepTime\") - F.col(\"CRSDepTime\")).alias(\"x\"), (F.col(\"ArrTime\") - F.col(\"CRSArrTime\")).alias(\"y\"))\n",
    ".toPandas())\n",
    "delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17a733f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regr = linear_model.LinearRegression()\n",
    "\n",
    "delays_x = np.array(delays[\"x\"].tolist()).reshape(-1, 1)\n",
    "delays_y = np.array(delays[\"y\"].tolist())\n",
    "\n",
    "linear_regr.fit(delays_x, delays_y)\n",
    "\n",
    "prediction = linear_regr.predict(delays_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b98151a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: 0.013495405202963329\n",
      "Y-Intercept: -20.148166581601433\n",
      "Slope: [0.22335476]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coefficient: {linear_regr.score(delays_x, delays_y)}\")\n",
    "print(f\"Y-Intercept: {linear_regr.intercept_}\")\n",
    "print(f\"Slope: {linear_regr.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67180f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 84150.74\n",
      "Coefficient of determination: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error: %.2f\" % mean_squared_error(delays[\"y\"], prediction))\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(delays[\"y\"], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "590bcd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(delays[\"x\"], delays[\"y\"], color=\"black\")\n",
    "plt.plot(delays[\"x\"], prediction, color=\"blue\", linewidth=2)\n",
    "\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71ee09",
   "metadata": {},
   "source": [
    "In conclusion, the departure delay (i.e., DepTime - CRSDepTime) predictive of the arrival delay (ArrTime - CRSArrTime). This is because the coefficient of determination is 0.01, or 1% which means that there are too many variants in the data. From the scatter plot, we can see that this is true as it does not follow a linear line where if the departure time is earlier then the arrival time should also be earlier, and vice versa."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f5ef9090ac90e070e55a0ce4d23d2597e2c77ac72a90d357fb2c4b017d4c72e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
